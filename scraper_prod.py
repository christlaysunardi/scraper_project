# -*- coding: utf-8 -*-
"""scraper_prod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BZGPkPzypD-luIiZY3WJC6VtbCNbFKhp
"""

# Install Google Chrome and Chromedriver (for Colab)
!apt-get update
!apt-get install -y wget unzip
!wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb || apt --fix-broken install -y
!apt install -y google-chrome-stable
!CHROME_VERSION=$(google-chrome-stable --version | grep -oP '\d+\.\d+\.\d+') \
 && wget -q https://chromedriver.storage.googleapis.com/$(echo $CHROME_VERSION)/chromedriver_linux64.zip \
 && unzip chromedriver_linux64.zip \
 && mv chromedriver /usr/local/bin/

!pip install streamlit selenium webdriver-manager beautifulsoup4 pandas

import streamlit as st
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# Area data dictionary
areas = {'Jakarta Utara':'DKI Jakarta','Jakarta Selatan':'DKI Jakarta','Jakarta Barat':'DKI Jakarta','Jakarta Timur':'DKI Jakarta','Jakarta Pusat':'DKI Jakarta','Bandung':'Jawa Barat','Bekasi':'Jawa Barat','Bogor':'Jawa Barat','Depok':'Jawa Barat','Cirebon':'Jawa Barat','Cikarang':'Jawa Barat','Cimahi':'Jawa Barat','Bandung Barat':'Jawa Barat','Karawang':'Jawa Barat','Cianjur':'Jawa Barat','Surabaya':'Jawa Timur','Malang':'Jawa Timur','Sidoarjo':'Jawa Timur','Gresik':'Jawa Timur','Kediri':'Jawa Timur','Jember':'Jawa Timur','Tulungagung':'Jawa Timur','Mojokerto':'Jawa Timur','Batu':'Jawa Timur','Madiun':'Jawa Timur','Tangerang':'Banten','Tangerang Selatan':'Banten','Serang':'Banten','Cilegon':'Banten','Lebak':'Banten','Pandeglang':'Banten','Semarang':'Jawa Tengah','Solo':'Jawa Tengah','Surakarta':'Jawa Tengah','Sukoharjo':'Jawa Tengah','Tegal':'Jawa Tengah','Karanganyar':'Jawa Tengah','Purwokerto':'Jawa Tengah','Magelang':'Jawa Tengah','Salatiga':'Jawa Tengah','Klaten':'Jawa Tengah','Badung':'Bali','Denpasar':'Bali','Gianyar':'Bali','Tabanan':'Bali','Buleleng':'Bali','Klungkung':'Bali','Karangasem':'Bali','Sleman':'Daerah Istimewa Yogyakarta','Yogyakarta':'Daerah Istimewa Yogyakarta','Bantul':'Daerah Istimewa Yogyakarta','Kulon Progo':'Daerah Istimewa Yogyakarta','Makassar':'Sulawesi Selatan','Gowa':'Sulawesi Selatan','Bulukumba':'Sulawesi Selatan','Pare-Pare':'Sulawesi Selatan','Bone':'Sulawesi Selatan','Tana Toraja':'Sulawesi Selatan','Luwu Timur':'Sulawesi Selatan','Wajo':'Sulawesi Selatan','Maros':'Sulawesi Selatan','Medan':'Sumatera Utara','Deli Serdang':'Sumatera Utara','Binjai':'Sumatera Utara','Tebing Tinggi':'Sumatera Utara','Pematang Siantar':'Sumatera Utara','Balikpapan':'Kalimantan Timur','Samarinda':'Kalimantan Timur','Bontang':'Kalimantan Timur','Kutai Kartanegara':'Kalimantan Timur','Penajam Paser Utara':'Kalimantan Timur','Batam':'Kepulauan Riau','Tanjung Pinang':'Kepulauan Riau','Pontianak':'Kalimantan Barat','Kubu Raya':'Kalimantan Barat','Singkawang':'Kalimantan Barat','Ketapang':'Kalimantan Barat','Sintang':'Kalimantan Barat','Bandar Lampung':'Lampung','Metro':'Lampung','Lampung Selatan':'Lampung','Pekanbaru':'Riau','Kampar':'Riau','Palembang':'Sumatera Selatan','Ogan Ilir':'Sumatera Selatan','Banyuasin':'Sumatera Selatan','Manado':'Sulawesi Utara','Minahasa Utara':'Sulawesi Utara','Bitung':'Sulawesi Utara','Tomohon':'Sulawesi Utara','Bolaang Mongondow':'Sulawesi Utara','Banjarmasin':'Kalimantan Selatan','Banjar Baru':'Kalimantan Selatan','Tanah Bumbu':'Kalimantan Selatan','Banjar':'Kalimantan Selatan','Tanah Laut':'Kalimantan Selatan','Kupang':'Nusa Tenggara Timur','Sikka':'Nusa Tenggara Timur','Ende':'Nusa Tenggara Timur','Belu':'Nusa Tenggara Timur','Timor Tengah Selatan':'Nusa Tenggara Timur','Manggarai':'Nusa Tenggara Timur','Manggarai Barat':'Nusa Tenggara Timur','Sumba Timur':'Nusa Tenggara Timur','Mimika':'Papua','Jayapura':'Papua','Dompu':'Nusa Tenggara Barat','Mataram':'Nusa Tenggara Barat','Lombok Utara':'Nusa Tenggara Barat','Lombok Timur':'Nusa Tenggara Barat','Palu':'Sulawesi Tengah','Ambon':'Maluku','Seram Bagian Barat':'Maluku','Pangkal Pinang':'Kepulauan Bangka Belitung','Bangka Selatan':'Kepulauan Bangka Belitung','Sorong':'Papua Barat','Manokwari':'Papua Barat','Padang':'Sumatera Barat','Gorontalo':'Gorontalo','Pulau Morotai':'Maluku Utara','Ternate':'Maluku Utara','Konawe Selatan':'Sulawesi Tenggara','Kendari':'Sulawesi Tenggara','Jambi':'Jambi','Palangkaraya':'Kalimantan Tengah','Bengkulu':'Bengkulu','Banda Aceh':'Aceh'}

# Streamlit UI
st.title("Rumah123 Ruko Scraper")

area_level = st.selectbox("Choose area level:", ["Province", "City"])

if area_level == "City":
    options_list = sorted(set(areas.keys()))
else:
    options_list = sorted(set(areas.values()))

scope = st.selectbox(f"Select {area_level.lower()}:", options_list)

if st.button("Start Scraping"):
    scope_id = scope.lower().replace(' ', '-')
    url_page1 = f"https://www.rumah123.com/sewa/{scope_id}/ruko/"
    base_url = f"https://www.rumah123.com/sewa/{scope_id}/ruko/?page="

    # Setup Chrome options for headless browsing
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--window-size=1920,1080')

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    def get_max_page(driver, url):
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[aria-label="Next page"]')))
        except:
            pass
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        next_page_a = soup.find('a', attrs={'aria-label': 'Next page'})
        max_page = 1
        if next_page_a:
            next_page_li = next_page_a.find_parent('li')
            if next_page_li:
                prev_li = next_page_li.find_previous_sibling('li')
                if prev_li:
                    a_tag = prev_li.find('a')
                    if a_tag and a_tag.text.strip().isdigit():
                        max_page = int(a_tag.text.strip())
        return min(max_page, 10)  # limit max pages to 10

    max_page = get_max_page(driver, url_page1)
    st.write(f"Max pages to scrape: {max_page}")

    url_list = [url_page1] + [base_url + str(i) for i in range(2, max_page + 1)]

    data = []
    max_retries = 2

    for url in url_list:
        for attempt in range(max_retries):
            try:
                driver.get(url)
                wait = WebDriverWait(driver, 5)
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.card-featured__middle-section')))
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                cards = soup.find_all('div', class_='card-featured__middle-section')
                if not cards:
                    raise Exception("No listings found on page")
                for card in cards:
                    title_tag = card.find('h2')
                    title = title_tag.text.strip() if title_tag else 'No Title'
                    link = 'No Link'
                    if title_tag:
                        a_tag = title_tag.find_parent('a')
                        if a_tag and a_tag.has_attr('href'):
                            href = a_tag['href']
                            link = f"https://www.rumah123.com{href}" if href.startswith('/') else href
                    price_tag = card.find('div', class_='card-featured__middle-section__price')
                    price = price_tag.text.strip() if price_tag else 'No Price'
                    location = 'No Location'
                    if title_tag:
                        span_tag = title_tag.find_next('span')
                        if span_tag:
                            location = span_tag.text.strip()
                    desc_tag = card.find('p')
                    description = desc_tag.text.strip() if desc_tag else 'No Description'
                    lt = lb = 'N/A'
                    attr_tags = card.find_all('div', class_='attribute-info')
                    for attr in attr_tags:
                        if 'LT' in attr.text:
                            lt = attr.find('span').text.strip()
                        if 'LB' in attr.text:
                            lb = attr.find('span').text.strip()
                    data.append({
                        "Title": title,
                        "Link": link,
                        "Price": price,
                        "Location": location,
                        "Description": description,
                        "LT": lt,
                        "LB": lb,
                        "Page_URL": url
                    })
                st.write(f"Scraped {len(cards)} listings from {url}")
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    st.error(f"Failed to scrape {url}: {e}")
                else:
                    time.sleep(3)

    driver.quit()

    df = pd.DataFrame(data)
    st.dataframe(df)

    csv = df.to_csv(index=False)
    st.download_button("Download CSV", data=csv, file_name=f"rumah123_{scope_id}.csv")